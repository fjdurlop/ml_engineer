
# Part 2: ASR Evaluation Framework

## Overview
You need to benchmark ASR models across different dialects and measure WER vs latency trade-offs.

## Code: asr_evaluation_framework.py

Initial code:
- SimpleASRModel
- ASREvaluator
- generate_test_dataset

## Tasks for Part 2

### 1. Comprehensive Evaluation Pipeline (60 mins)
Build an evaluation system that measures:

**Performance Metrics:**
- WER per dialect and overall
- Character Error Rate (CER)
- Real-time factor (RTF)
- 95th percentile latency

**Quality Analysis:**
- Error breakdown by phoneme/word type
- Performance correlation with audio duration
- Statistical significance testing across dialects


### Research and Analysis Skills
- **Benchmarking Methodology:** Proper experimental design and statistical analysis
- **Performance Characterization:** Understanding of model behavior across conditions
- **Documentation:** Clear explanation of findings and recommendations


**File: `comprehensive_asr_evaluation.py`**
- Enhanced ASR evaluation framework
- Statistical analysis and significance testing
- Model comparison and selection strategy
- Performance vs accuracy trade-off analysis



# report

Initial code:
- SimpleASRModel
- ASREvaluator
- generate_test_dataset


Inference run:
- forward call
  - feature_extractor
  - self.encoder(features)
  - logits = self.classifier(encoded)
  - log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
- decode log_probs

See README.md to run experiments

## System

- Evaluate per model and dialect
- Statistical analysis
  - Statistical graphs
  - Statistical tests
  - Automated conclusions 


## Data analysis:

[Durán, F., Martinez, M., Lago, P., & Martínez-Fernández, S. (2025). Insights into resource utilization of code small language models serving with runtime engines and execution providers. Journal of Systems and Software, 112574.]
For each hypothesis, we follow a systematic analysis process: 
(1)
use box plots to illustrate the distributions for each dependent variable,
comparing between configurations using the same model; 
(2) 
assess if the measurements are normally distributed and have equal variances
across the different treatments of each RQ. Utilize the Shapiro–Wilk
test to check for the normality of the data. To check the homogeneity
of variances, we use a Levene test for equality of variances. 
(3) 
assess the statistical significance (i.e., p-value) of the findings.

In our data analysis, we conducted two types of statistical tests
based on the distribution of the dependent variables. 
For configurations where the data is normally distributed, we applied Welch’s ANOVA to
assess differences between serving configurations, since the dependent
variables do not exhibit equality of variances, followed by the Games-
Howell test for post-hoc analysis. For non-normally distributed data, we
used the Kruskal–Wallis test, with Dunn’s test as the post-hoc analysis to
identify specific differences between groups. The details of the analysis
and test results are available in the replication package for further
examination (see data availability statement in Section 1).