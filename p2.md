
# Part 2: ASR Evaluation Framework

## Table of contens
- [Part 2: ASR Evaluation Framework](#part-2-asr-evaluation-framework)
  - [Table of contens](#table-of-contens)
  - [Overview](#overview)
  - [Code: asr\_evaluation\_framework.py](#code-asr_evaluation_frameworkpy)
  - [Tasks for Part 2](#tasks-for-part-2)
    - [1. Comprehensive Evaluation Pipeline (60 mins)](#1-comprehensive-evaluation-pipeline-60-mins)
    - [Research and Analysis Skills](#research-and-analysis-skills)
- [report](#report)
  - [System](#system)
  - [Model comparison and selection strategy](#model-comparison-and-selection-strategy)
  - [Performance vs accuracy trade-off analysis](#performance-vs-accuracy-trade-off-analysis)
  - [Data analysis:](#data-analysis)

## Overview
You need to benchmark ASR models across different dialects and measure WER vs latency trade-offs.

## Code: asr_evaluation_framework.py

Initial code:
- SimpleASRModel
- ASREvaluator
- generate_test_dataset

## Tasks for Part 2

### 1. Comprehensive Evaluation Pipeline (60 mins)
Build an evaluation system that measures:

**Performance Metrics:**
- WER per dialect and overall
- Character Error Rate (CER)
- Real-time factor (RTF)
- 95th percentile latency

**Quality Analysis:**
- Error breakdown by phoneme/word type
- Performance correlation with audio duration
- Statistical significance testing across dialects


### Research and Analysis Skills
- **Benchmarking Methodology:** Proper experimental design and statistical analysis
- **Performance Characterization:** Understanding of model behavior across conditions
- **Documentation:** Clear explanation of findings and recommendations


**File: `comprehensive_asr_evaluation.py`**
- Enhanced ASR evaluation framework
- Statistical analysis and significance testing
- Model comparison and selection strategy
- Performance vs accuracy trade-off analysis



# report

Initial code:
- SimpleASRModel
- ASREvaluator
- generate_test_dataset


Inference run:
- forward call
  - feature_extractor
  - self.encoder(features)
  - logits = self.classifier(encoded)
  - log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
- decode log_probs

See README.md to run experiments

## System

- Evaluate per model and dialect
- Statistical analysis
  - Statistical graphs
  - Statistical tests
  - Automated conclusions 

## Model comparison and selection strategy
In order to compare and select the most suitable ASR models for deployment, we can follow a systematic, multi-step process:
1. Metric aggregation: compute per-model metrics (WER, CER, real-time factor, P95 latency) across dialects, summarized in tables.
2. Get candidates that meet the SLOs
3. Visualization: generate radar charts and bar plots to illustrate
4. Statistical validation: apply ANOVA or Kruskal–Wallis tests with post-hoc analysis to confirm significant differences among top candidates.


Selection strategy:
- Filter models using SLOs, e.g., WER ≤ X%, latency ≤ Y ms.
- Rank filtered models by prioritized metrics according to the use case (higher latency, higher WER, ...)
- Shortlist top K models for final evaluation or A/B testing.

## Performance vs accuracy trade-off analysis
To analyze the trade-off between recognition accuracy and processing speed:
- Create scatter plots of WER (y-axis) vs P95 latency (x-axis) for all models.
- Highlight points that represent optimal trade-offs.
- Annotate regions corresponding to acceptable operating points based on deployment constraints.
- Provide decision guidelines, for example: 
  - "For real-time streaming (P95 latency ≤ 500 ms), select models with WER ≤ 10%"
  - "For audio processing (no real time) (P95 latency ≤ 2000 ms), select models with WER ≤ 5%"



## Data analysis:

[Durán, F., Martinez, M., Lago, P., & Martínez-Fernández, S. (2025). Insights into resource utilization of code small language models serving with runtime engines and execution providers. Journal of Systems and Software, 112574.]
For each hypothesis, we follow a systematic analysis process: 
(1)
use box plots to illustrate the distributions for each dependent variable,
comparing between configurations using the same model; 
(2) 
assess if the measurements are normally distributed and have equal variances
across the different treatments of each RQ. Utilize the Shapiro–Wilk
test to check for the normality of the data. To check the homogeneity
of variances, we use a Levene test for equality of variances. 
(3) 
assess the statistical significance (i.e., p-value) of the findings.

In our data analysis, we conducted two types of statistical tests
based on the distribution of the dependent variables. 
For configurations where the data is normally distributed, we applied Welch’s ANOVA to
assess differences between serving configurations, since the dependent
variables do not exhibit equality of variances, followed by the Games-
Howell test for post-hoc analysis. For non-normally distributed data, we
used the Kruskal–Wallis test, with Dunn’s test as the post-hoc analysis to
identify specific differences between groups. The details of the analysis
and test results are available in the replication package for further
examination (see data availability statement in Section 1).