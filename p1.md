# Part 1: TTS Optimization Challenge

## Overview
You're given a PyTorch TTS model that's too slow for production. Your task is to optimize it for real-time inference.

## Current Performance Baseline
- **Latency:** 800ms average for 50-character input
- **Throughput:** 15 samples/second on V100
- **Memory:** 4GB GPU memory usage
- **Target:** <200ms latency, >50 samples/second

## Code: baseline_inference.py

## Tasks for Part 1

### 1. Performance Analysis (45 mins)
Using the provided profiling tools:
1. Identify the top 3 bottlenecks in the inference pipeline
2. Measure GPU utilization and memory bandwidth
3. Analyze batch size vs latency trade-offs
4. Document findings with concrete metrics

### 2. Optimization Implementation (90 mins)
Implement 3 optimization techniques:
1. **Model quantization** (INT8 or mixed precision)
2. **Continuous batching** for variable-length inputs
3. **KV-cache optimization** for autoregressive generation

**Requirements:**
- Maintain audio quality (MOS score within 5% of baseline)
- Measure latency/throughput improvements
- Handle edge cases (very short/long inputs)

### 3. Production Deployment (15 mins)
Design a deployment strategy addressing:
- How to handle multiple concurrent requests
- Memory management across different GPU types
- Fallback strategies for optimization failures




## Reporting

exp1
bottleneck:
- all runs are sequential

- Comparison 
  - benchmark_baseline->synthesize, 
    - expected results
      - nums_runs * test_texts - sequential , 80 sequential runs
    - test_texts
    - num_runs=8
    - batch_sizes=1
  - batch_synthesize, inputs (sequential) ->  
    - expected results
      - b1, b_size * test_texts*8 - sequential , 80 sequential runs
      - b2, 2 * test_texts*8/2 - sequential , 80 sequential runs
      - b2, 4 * test_texts*8/4 - sequential , 80 sequential runs
    - test_texts*8
    - num_runs=1
    - batch_sizes=1,2,4
  
setup: original batch synthesize, original synthesize

Baseline Results:
  Num of requests: 80.000s
  Average Latency: 2.788s
  95th Percentile Latency: 2.946s
  Average RTF: 15.369
  Throughput: 0.4 samples/second

Batch size 1:
  Avg latency: 2.7941 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 98.0%

Batch size 2:
  Avg latency: 5.5984 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 97.7%

Batch size 4:
  Avg latency: 11.1791 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 98.0%

exp3

without profiler and profiler

exp4

- profiling per section
  - text_to_tokens, this encodes the text input into tokens
  - encode_text, this encodes the tokens into text features
  - decode_mel, this decodes the text features into mel spectrograms
  - vocoder_inference, this decodes the mel spectrograms into audio waveforms
  
decoder_mel is the bottleneck, it grows with the length of the input text, because it is autoregressive, so we can do KV cache



exp5
still per section, with load model

exp6 length



expbatch 1
 see exp2
when immediate, low-latency results aren't necessary, but high throughput and cost-effectiveness are crucial for processing large datasets.


expbatch 4 - quantization
  exp_batch_q

exp_batch_eff_4:
- eff_4, tensor copies
- eff_3, cuda.synchronize issue

+ 2 hrs
 8 -rw-rw-r-- 1 ubuntu ubuntu  6342 Sep 24 22:16 exp_batch_eff_4_4.out
 4 drwxrwxr-x 2 ubuntu ubuntu  4096 Sep 24 22:16 .
28 -rw-rw-r-- 1 ubuntu ubuntu 25132 Sep 24 22:14 exp_batch_eff_4_3.out
 4 drwxrwxr-x 9 ubuntu ubuntu  4096 Sep 24 22:03 ..
 8 -rw-rw-r-- 1 ubuntu ubuntu  5135 Sep 24 21:54 exp_batch_eff_4_2.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  3414 Sep 24 21:44 exp_batch_eff_4_1.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2695 Sep 24 21:30 exp_batch_q_4_2.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   730 Sep 24 21:28 exp_batch_q_4.out
40 -rw-rw-r-- 1 ubuntu ubuntu 38552 Sep 24 19:00 exp_batch_3.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  1060 Sep 24 18:38 exp_batch_2.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   761 Sep 24 18:36 exp_batch_1.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2083 Sep 24 18:30 exp6_2_10runs.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   716 Sep 24 18:23 exp6_2_long.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   717 Sep 24 18:22 exp6_2_medium.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   713 Sep 24 18:21 exp6_2_short.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2073 Sep 24 18:18 exp6_2.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2256 Sep 24 18:10 exp6.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  1018 Sep 24 17:56 exp5.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   916 Sep 24 17:49 exp4.out
 0 -rw-rw-r-- 1 ubuntu ubuntu     0 Sep 24 17:41 exp3.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2364 Sep 24 16:01 exp2.out
16 -rw-rw-r-- 1 ubuntu ubuntu 14834 Sep 24 15:51 exp1.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   151 Sep 24 14:12 improved.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   475 Sep 24 12:50 baseline_inference.out