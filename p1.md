# Part 1: TTS Optimization Challenge

## Overview
You're given a PyTorch TTS model that's too slow for production. Your task is to optimize it for real-time inference.

## Current Performance Baseline
- **Latency:** 800ms average for 50-character input
- **Throughput:** 15 samples/second on V100
- **Memory:** 4GB GPU memory usage
- **Target:** <200ms latency, >50 samples/second

## Code: baseline_inference.py

## Tasks for Part 1

### 1. Performance Analysis (45 mins)
Using the provided profiling tools:
1. Identify the top 3 bottlenecks in the inference pipeline
2. Measure GPU utilization and memory bandwidth
3. Analyze batch size vs latency trade-offs
4. Document findings with concrete metrics

### 2. Optimization Implementation (90 mins)
Implement 3 optimization techniques:
1. **Model quantization** (INT8 or mixed precision)
2. **Continuous batching** for variable-length inputs
3. **KV-cache optimization** for autoregressive generation

**Requirements:**
- Maintain audio quality (MOS score within 5% of baseline)
- Measure latency/throughput improvements
- Handle edge cases (very short/long inputs)

### 3. Production Deployment (15 mins)
Design a deployment strategy addressing:
- How to handle multiple concurrent requests
- Memory management across different GPU types
- Fallback strategies for optimization failures




## Report

### Framework

- [x] profile
- [ ] computational complexity analysis
- [x] bottlenecks
- [x] optimize bottlenecks
- [ ] compute complexity analysis after optimizations
- [x] profile after optimizations
- [x] compare results
- [x] report

Notes:
profile using length different inputs - if it growths a lot with input length, then it is a bottleneck, might be the decoder, it depends on the length of the output sequence, then we can do KV cache

## 

exp1
bottleneck:
- all runs are sequential

- Comparison 
  - benchmark_baseline->synthesize, 
    - expected results
      - nums_runs * test_texts - sequential , 80 sequential runs
    - test_texts
    - num_runs=8
    - batch_sizes=1
  - batch_synthesize, inputs (sequential) ->  
    - expected results
      - b1, b_size * test_texts*8 - sequential , 80 sequential runs
      - b2, 2 * test_texts*8/2 - sequential , 80 sequential runs
      - b2, 4 * test_texts*8/4 - sequential , 80 sequential runs
    - test_texts*8
    - num_runs=1
    - batch_sizes=1,2,4
  
setup: original batch synthesize, original synthesize

Baseline Results:
  Num of requests: 80.000s
  Average Latency: 2.788s
  95th Percentile Latency: 2.946s
  Average RTF: 15.369
  Throughput: 0.4 samples/second

Batch size 1:
  Avg latency: 2.7941 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 98.0%

Batch size 2:
  Avg latency: 5.5984 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 97.7%

Batch size 4:
  Avg latency: 11.1791 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 98.0%

exp3

without profiler and profiler

exp4

- profiling per section
  - text_to_tokens, this encodes the text input into tokens
  - encode_text, this encodes the tokens into text features
  - decode_mel, this decodes the text features into mel spectrograms
  - vocoder_inference, this decodes the mel spectrograms into audio waveforms
  
decoder_mel is the bottleneck, it grows with the length of the input text, because it is autoregressive, so we can do KV cache



exp5
still per section, with load model

exp6 length



expbatch 1
 see exp2
when immediate, low-latency results aren't necessary, but high throughput and cost-effectiveness are crucial for processing large datasets.


expbatch 4 - quantization
  exp_batch_q

exp_batch_eff_4:
- eff_4, tensor copies
- eff_3, cuda.synchronize issue

+ 2 hrs
 8 -rw-rw-r-- 1 ubuntu ubuntu  6342 Sep 24 22:16 exp_batch_eff_4_4.out
 4 drwxrwxr-x 2 ubuntu ubuntu  4096 Sep 24 22:16 .
28 -rw-rw-r-- 1 ubuntu ubuntu 25132 Sep 24 22:14 exp_batch_eff_4_3.out
 4 drwxrwxr-x 9 ubuntu ubuntu  4096 Sep 24 22:03 ..
 8 -rw-rw-r-- 1 ubuntu ubuntu  5135 Sep 24 21:54 exp_batch_eff_4_2.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  3414 Sep 24 21:44 exp_batch_eff_4_1.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2695 Sep 24 21:30 exp_batch_q_4_2.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   730 Sep 24 21:28 exp_batch_q_4.out
40 -rw-rw-r-- 1 ubuntu ubuntu 38552 Sep 24 19:00 exp_batch_3.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  1060 Sep 24 18:38 exp_batch_2.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   761 Sep 24 18:36 exp_batch_1.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2083 Sep 24 18:30 exp6_2_10runs.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   716 Sep 24 18:23 exp6_2_long.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   717 Sep 24 18:22 exp6_2_medium.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   713 Sep 24 18:21 exp6_2_short.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2073 Sep 24 18:18 exp6_2.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2256 Sep 24 18:10 exp6.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  1018 Sep 24 17:56 exp5.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   916 Sep 24 17:49 exp4.out
 0 -rw-rw-r-- 1 ubuntu ubuntu     0 Sep 24 17:41 exp3.out
 4 -rw-rw-r-- 1 ubuntu ubuntu  2364 Sep 24 16:01 exp2.out
16 -rw-rw-r-- 1 ubuntu ubuntu 14834 Sep 24 15:51 exp1.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   151 Sep 24 14:12 improved.out
 4 -rw-rw-r-- 1 ubuntu ubuntu   475 Sep 24 12:50 baseline_inference.out



## todo
1. **Model quantization** (INT8 or mixed precision)
2. **Continuous batching** for variable-length inputs
3. **KV-cache optimization** for autoregressive generation


# Report


## Profile baseline

device: V100
- Baseline Results:
  Num of requests: 80.000s
  Average Latency: 2.788s
  95th Percentile Latency: 2.946s
  Average RTF: 15.369
  Throughput: 0.4 samples/second

Device: Tesla T4
- Baseline Results:
- Num of requests: 50
- Average Latency: 4.5s
- 95th Percentile Latency: 4.8s
- Average RTF: 25.0
- Throughput: 0.22 samples/second

What to profile:
analysing forward pass, per section


synthesize
- text_to_tokens()
- model()
  - encode_text()
  - decode_mel()
  - vocoder_inference()
- cuda.synchronize()
- post_process()


[todo] computational complexity analysis
T = text_len
M = mel_len
...

Bottlenecks:
- 1. no batching - processing samples individually
- 2. full precision (FP32) - no quantization
  - 1. **Model quantization** (INT8 or mixed precision)
  - Quantize neural TTS models for minimal latency on heterogeneous GPU hardware
  - 
- 3. synchronous GPU operations - cuda.synchronize() forces CPU to wait for GPU
- 4. inefficient tensor copies - unnecessary transfers between CPU and GPU
- 5. No caching
  - 3. **KV-cache optimization** for autoregressive generation

From profiling per section:

top per time:
- decode_mel

per memory:
- encode_text
- decode_mel
- vocoder_inference

per gpu utilization: using more than 50%
- decode_mel
- vocoder_inference
- synchronize
- to_cpu

comparison of implemented batching vs original batching:

uv run baseline_inference_original.py --num_runs 2 --batch_sizes 2 4 8 2>&1 | tee logs/p1/baseline_inference_original_2_compare_new_batching.out


quantization
- problems
  - when changing verify all operations are casted

added policy detection according to GPU capability
[todo] verify cpu int8 quantization

(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 2 2>&1 | tee logs/p1/baseline_inference_original_3.out



### kv cache

from logs/profiling_per_section_1_2.png:
- bottleneck: decode_mel more time and gpu utilization
- Autoregressive mel spectrogram generation
aut


short vs long 
(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 20 2>&1 | tee logs/p1/baseline_inference_original_5_short.out

(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 20 2>&1 | tee logs/p1/baseline_inference_original_5_long.out

logs/profiling_per_section_5_cache_long_no_cache.png

With larger texts is more notable the improvement


### final comparison with all optimizations:
Sequential serving

- precision
  - decode_mel
- copies and intermediate results
  - synthesize_test
- kv cache

Baseline Results:
  Num of requests: 100
  Average Latency: 0.274s
  95th Percentile Latency: 0.351s
  Average RTF: 754.570
  Throughput: 3.7 samples/second

Improved:
  Baseline Results:
  Num of requests: 100
  Average Latency: 0.140s
  95th Percentile Latency: 0.148s
  Average RTF: 386.940
  Throughput: 7.1 samples/second


### further improvements
todo: Batching with same improvements
Fallback to CPU if not gpu
