# Part 1: TTS Optimization Challenge

## Overview
You're given a PyTorch TTS model that's too slow for production. Your task is to optimize it for real-time inference.

## Current Performance Baseline
- **Latency:** 800ms average for 50-character input
- **Throughput:** 15 samples/second on V100
- **Memory:** 4GB GPU memory usage
- **Target:** <200ms latency, >50 samples/second

## Code: baseline_inference.py

## Tasks for Part 1

### 1. Performance Analysis (45 mins)
Using the provided profiling tools:
1. Identify the top 3 bottlenecks in the inference pipeline
2. Measure GPU utilization and memory bandwidth
3. Analyze batch size vs latency trade-offs
4. Document findings with concrete metrics

### 2. Optimization Implementation (90 mins)
Implement 3 optimization techniques:
1. **Model quantization** (INT8 or mixed precision)
2. **Continuous batching** for variable-length inputs
3. **KV-cache optimization** for autoregressive generation

**Requirements:**
- Maintain audio quality (MOS score within 5% of baseline)
- Measure latency/throughput improvements
- Handle edge cases (very short/long inputs)

### 3. Production Deployment (15 mins)
Design a deployment strategy addressing:
- How to handle multiple concurrent requests
- Memory management across different GPU types
- Fallback strategies for optimization failures




# Report

## Framework

- [x] profile
- [ ] computational complexity analysis
- [x] bottlenecks
- [x] optimize bottlenecks
- [ ] compute complexity analysis after optimizations
- [x] profile after optimizations
- [x] compare results
- [x] report

Notes:
profile using length different inputs - if it growths a lot with input length, then it is a bottleneck, might be the decoder, it depends on the length of the output sequence, then we can do KV cache

## final comparison with all optimizations:


### Example 1:
Baseline Results:
  Num of requests: 100
  Average Latency: 0.274s
  95th Percentile Latency: 0.351s
  Average RTF: 754.570
  Throughput: 3.7 samples/second

Improved results:
  Baseline Results:
  Num of requests: 100
  Average Latency: 0.140s
  95th Percentile Latency: 0.148s
  Average RTF: 386.940
  Throughput: 7.1 samples/second

### Example 2

logs_dev/p1/baseline_inference.out
logs_dev/p1/baseline_inference_batching.out

graphs:
logs_dev/profiling_per_section_last_improved_6_original.png
logs_dev/profiling_per_section_last_improved_6_improved.png


#### Baseline
Baseline Results:
  Num of requests: 10
  Average Latency: 2.781s
  95th Percentile Latency: 3.264s
  Average RTF: 85.068
  Throughput: 0.4 samples/second

Profiling Results:
Section: text_to_tokens
  Latency (s): 0.020573157530564528
  Memory delta (MB): 0.0010892427884615385
  GPU utilization (%): 30.923076923076923
Section: encode_text
  Latency (s): 0.031051727441641
  Memory delta (MB): 0.8452524038461539
  GPU utilization (%): 12.846153846153847
Section: decode_mel
  Latency (s): 2.3804782720712514
  Memory delta (MB): 0.13112229567307693
  GPU utilization (%): 82.61538461538461
Section: vocoder_inference
  Latency (s): 0.027376376665555514
  Memory delta (MB): 0.013296274038461538
  GPU utilization (%): 78.23076923076923
Section: synchronize
  Latency (s): 0.020638557580801156
  Memory delta (MB): 0.0
  GPU utilization (%): 74.23076923076923
Section: to_cpu
  Latency (s): 0.021482119193443887
  Memory delta (MB): 0.0
  GPU utilization (%): 74.3076923076923

#### Improved version
Results:
  Num of requests: 10
  Average Latency: 0.677s
  95th Percentile Latency: 2.534s
  Average RTF: 289.473
  Throughput: 1.5 samples/second

Profiling Results:
Section: text_to_tokens
  Latency (s): 0.0003561973571777344
  Memory delta (MB): 0.00126953125
  GPU utilization (%): nan
Section: encode_text
  Latency (s): 0.03562948703765869
  Memory delta (MB): 38.246875
  GPU utilization (%): 3.9
Section: decode_mel
  Latency (s): 0.5027035236358642
  Memory delta (MB): 31.12685546875
  GPU utilization (%): 6.1
Section: vocoder_inference
  Latency (s): 0.028743481636047362
  Memory delta (MB): 0.41064453125
  GPU utilization (%): 6.2
Section: synchronize
  Latency (s): nan
  Memory delta (MB): nan
  GPU utilization (%): nan
Section: to_cpu
  Latency (s): 0.021608424186706544
  Memory delta (MB): 0.0
  GPU utilization (%): 5.2

## experiment notes


setup: original batch synthesize, original synthesize

Baseline Results:
  Num of requests: 80.000s
  Average Latency: 2.788s
  95th Percentile Latency: 2.946s
  Average RTF: 15.369
  Throughput: 0.4 samples/second

Batch size 1:
  Avg latency: 2.7941 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 98.0%

Batch size 2:
  Avg latency: 5.5984 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 97.7%

Batch size 4:
  Avg latency: 11.1791 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 98.0%


exp4

- profiling per section
  - text_to_tokens, this encodes the text input into tokens
  - encode_text, this encodes the tokens into text features
  - decode_mel, this decodes the text features into mel spectrograms
  - vocoder_inference, this decodes the mel spectrograms into audio waveforms
  
decoder_mel is the bottleneck, it grows with the length of the input text, because it is autoregressive, so we can do KV cache



batching:
when immediate, low-latency results aren't necessary, but high throughput and cost-effectiveness are crucial for processing large datasets.



Optimizations implemented:
- able to run batches
- automated quantization according to device
  - supported: fp32, fp16
- improvements in memory management
- kv caching in decoding step, reducing quadratic inference time to linear

device: V100
- Baseline Results:
  Num of requests: 80.000s
  Average Latency: 2.788s
  95th Percentile Latency: 2.946s
  Average RTF: 15.369
  Throughput: 0.4 samples/second

Device: Tesla T4
- Baseline Results:
- Num of requests: 50
- Average Latency: 4.5s
- 95th Percentile Latency: 4.8s
- Average RTF: 25.0
- Throughput: 0.22 samples/second

What to profile:
analysing forward pass, per section


synthesize
- text_to_tokens()
- model()
  - encode_text()
  - decode_mel()
  - vocoder_inference()
- cuda.synchronize()
- post_process()


[todo] computational complexity analysis
T = text_len
M = mel_len
...

Bottlenecks in baseline:
- 1. no batching - processing samples individually
- 2. full precision (FP32) - no quantization
  - 1. **Model quantization** (INT8 or mixed precision)
  - Quantize neural TTS models for minimal latency on heterogeneous GPU hardware
- 3. synchronous GPU operations - cuda.synchronize() forces CPU to wait for GPU
- 4. inefficient tensor copies - unnecessary transfers between CPU and GPU
- 5. No caching
  - 3. **KV-cache optimization** for autoregressive generation

From profiling per section:

top per time:
- decode_mel

per memory:
- encode_text
- decode_mel
- vocoder_inference

per gpu utilization: using more than 50%
- decode_mel
- vocoder_inference
- synchronize
- to_cpu

comparison of implemented batching vs original batching:

uv run baseline_inference_original.py --num_runs 2 --batch_sizes 2 4 8 2>&1 | tee logs/p1/baseline_inference_original_2_compare_new_batching.out


quantization
- note:
  - when changing verify all operations are casted

added policy detection according to GPU capability
[todo] verify cpu int8 quantization

(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 2 2>&1 | tee logs/p1/baseline_inference_original_3.out



### kv cache

from logs/profiling_per_section_1_2.png:
- bottleneck: decode_mel more time and gpu utilization
- Autoregressive mel spectrogram generation
aut


short vs long 
(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 20 2>&1 | tee logs/p1/baseline_inference_original_5_short.out

(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 20 2>&1 | tee logs/p1/baseline_inference_original_5_long.out

logs/profiling_per_section_5_cache_long_no_cache.png

With larger texts is more notable the improvement





## further improvements
todo: Batching with same improvements
Fallback to CPU if not gpu

[Zhou, Z., Ning, X., Hong, K., Fu, T., Xu, J., Li, S., ... & Wang, Y. (2024). A survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294.]
Explore Optimizations:
- System-level
  - Do not touch the model
  - Inference engines
    - Use open source optimized inference engines, e.g. Triton Inference Server, TorchServe, ...
  - Profile and optimize kernels, e.g. using nsys nvidia tool
  - Distributed systems
- Model-level
  - Model compression techniques, prunning layers, nodes
  - Dynamic inference (Different paths through the model according to input)
- Data-level
  - RAG

