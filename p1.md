# Part 1: TTS Optimization Challenge
## Table of Contents
- [Part 1: TTS Optimization Challenge](#part-1-tts-optimization-challenge)
  - [Table of Contents](#table-of-contents)
  - [Overview](#overview)
  - [Current Performance Baseline](#current-performance-baseline)
  - [Code: baseline\_inference.py](#code-baseline_inferencepy)
  - [Tasks for Part 1](#tasks-for-part-1)
    - [1. Performance Analysis (45 mins)](#1-performance-analysis-45-mins)
    - [2. Optimization Implementation (90 mins)](#2-optimization-implementation-90-mins)
    - [3. Production Deployment (15 mins)](#3-production-deployment-15-mins)
- [Report](#report)
  - [Framework](#framework)
  - [Final Comparison with All Optimizations](#final-comparison-with-all-optimizations)
    - [Example 1](#example-1)
    - [Example 2](#example-2)
      - [Profiling Results (Baseline)](#profiling-results-baseline)
      - [Profiling Results (Improved)](#profiling-results-improved)
  - [Computational Complexity Analysis](#computational-complexity-analysis)
  - [Experiment Notes](#experiment-notes)
    - [Batch Size vs Latency](#batch-size-vs-latency)
    - [Profiling Per Section (Exp4)](#profiling-per-section-exp4)
    - [batching:](#batching)
    - [Optimizations implemented](#optimizations-implemented)
    - [Inference Pipeline Breakdown](#inference-pipeline-breakdown)
    - [quantization](#quantization)
    - [KV Cache](#kv-cache)
  - [Further Improvements](#further-improvements)

## Overview
You're given a PyTorch TTS model that's too slow for production. Your task is to optimize it for real-time inference.

## Current Performance Baseline

| Metric     | Value                                     |
| ---------- | ----------------------------------------- |
| Latency    | 800 ms average for 50-character input     |
| Throughput | 15 samples/second on V100                 |
| Memory     | 4 GB GPU memory usage                     |
| Target     | < 200 ms latency; > 50 samples/second      |

## Code: baseline_inference.py

## Tasks for Part 1

### 1. Performance Analysis (45 mins)
Using the provided profiling tools:
1. Identify the top 3 bottlenecks in the inference pipeline
2. Measure GPU utilization and memory bandwidth
3. Analyze batch size vs latency trade-offs
4. Document findings with concrete metrics

### 2. Optimization Implementation (90 mins)
Implement 3 optimization techniques:
1. **Model quantization** (INT8 or mixed precision)
2. **Continuous batching** for variable-length inputs
3. **KV-cache optimization** for autoregressive generation

**Requirements:**
- Maintain audio quality (MOS score within 5% of baseline)
- Measure latency/throughput improvements
- Handle edge cases (very short/long inputs)

### 3. Production Deployment (15 mins)
Design a deployment strategy addressing:
- How to handle multiple concurrent requests
- Memory management across different GPU types
- Fallback strategies for optimization failures




# Report

## Framework

- [x] profile
- [ ] computational complexity analysis
- [x] bottlenecks
- [x] optimize bottlenecks
- [ ] compute complexity analysis after optimizations
- [x] profile after optimizations
- [x] compare results
- [x] report

Notes:
Profile using inputs of varying length, if latency grows significantly with input length, the decoder becomes the bottleneck due to autoregressive generation, motivating KV cache optimization.

## Final Comparison with All Optimizations


### Example 1

| Metric                  | Baseline (100 requests) | Improved (100 requests) |
| ----------------------- | ----------------------- | ----------------------- |
| Average Latency         | 0.274 s                 | 0.140 s                 |
| 95th Percentile Latency | 0.351 s                 | 0.148 s                 |
| Average RTF             | 754.570                 | 386.940                 |
| Throughput              | 3.7 samples/s           | 7.1 samples/s           |

### Example 2

**Logs:**
- [Baseline inference log](logs_dev/p1/baseline_inference.out)
- [Batching inference log](logs_dev/p1/baseline_inference_batching.out)

**Graphs:**
- [Baseline inference profiling per section](logs_dev/profiling_per_section_last_improved_6_original.png)
- [Improved inference profiling per section](logs_dev/profiling_per_section_last_improved_6_improved.png)


| Metric                  | Baseline (10 requests) | Improved (10 requests) |
| ----------------------- | ---------------------- | ---------------------- |
| Average Latency         | 2.781 s                | 0.677 s                |
| 95th Percentile Latency | 3.264 s                | 2.534 s                |
| Average RTF             | 85.068                 | 289.473                |
| Throughput              | 0.4 samples/s          | 1.5 samples/s          |

#### Profiling Results (Baseline)

```yaml
Section: text_to_tokens
  Latency (s): 0.020573157530564528
  Memory delta (MB): 0.0010892427884615385
  GPU utilization (%): 30.923076923076923
Section: encode_text
  Latency (s): 0.031051727441641
  Memory delta (MB): 0.8452524038461539
  GPU utilization (%): 12.846153846153847
Section: decode_mel
  Latency (s): 2.3804782720712514
  Memory delta (MB): 0.13112229567307693
  GPU utilization (%): 82.61538461538461
Section: vocoder_inference
  Latency (s): 0.027376376665555514
  Memory delta (MB): 0.013296274038461538
  GPU utilization (%): 78.23076923076923
Section: synchronize
  Latency (s): 0.020638557580801156
  Memory delta (MB): 0.0
  GPU utilization (%): 74.23076923076923
Section: to_cpu
  Latency (s): 0.021482119193443887
  Memory delta (MB): 0.0
  GPU utilization (%): 74.3076923076923
```

#### Profiling Results (Improved)

```yaml
Section: text_to_tokens
  Latency (s): 0.0003561973571777344
  Memory delta (MB): 0.00126953125
  GPU utilization (%): nan
Section: encode_text
  Latency (s): 0.03562948703765869
  Memory delta (MB): 38.246875
  GPU utilization (%): 3.9
Section: decode_mel
  Latency (s): 0.5027035236358642
  Memory delta (MB): 31.12685546875
  GPU utilization (%): 6.1
Section: vocoder_inference
  Latency (s): 0.028743481636047362
  Memory delta (MB): 0.41064453125
  GPU utilization (%): 6.2
Section: synchronize
  Latency (s): nan
  Memory delta (MB): nan
  GPU utilization (%): nan
Section: to_cpu
  Latency (s): 0.021608424186706544
  Memory delta (MB): 0.0
  GPU utilization (%): 5.2
```

## Computational Complexity Analysis

TODO: Provide theoretical analysis of time complexity as functions of text length (T) and mel length (M) before and after optimizations.

## Experiment Notes


### Batch Size vs Latency


setup: original batch synthesize, original synthesize

Baseline Results:
  Num of requests: 80.000s
  Average Latency: 2.788s
  95th Percentile Latency: 2.946s
  Average RTF: 15.369
  Throughput: 0.4 samples/second

Batch size 1:
  Avg latency: 2.7941 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 98.0%

Batch size 2:
  Avg latency: 5.5984 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 97.7%

Batch size 4:
  Avg latency: 11.1791 s
  Throughput: 0.36 samples/s
  GPU memory delta: 0.00 MB
  GPU utilization: 98.0%


### Profiling Per Section (Exp4)
- profiling per section
  - text_to_tokens, this encodes the text input into tokens
  - encode_text, this encodes the tokens into text features
  - decode_mel, this decodes the text features into mel spectrograms
  - vocoder_inference, this decodes the mel spectrograms into audio waveforms
  
decoder_mel is the bottleneck, it grows with the length of the input text, because it is autoregressive, so we can do KV cache



### batching:
when immediate, low-latency results aren't necessary, but high throughput and cost-effectiveness are crucial for processing large datasets.

comparison of implemented batching vs original batching:

uv run baseline_inference_original.py --num_runs 2 --batch_sizes 2 4 8 2>&1 | tee logs/p1/baseline_inference_original_2_compare_new_batching.out



### Optimizations implemented
- able to run batches
- automated quantization according to device
  - supported: fp32, fp16
- improvements in memory management
- kv caching in decoding step, reducing quadratic inference time to linear

device: V100
- Baseline Results:
  Num of requests: 80.000s
  Average Latency: 2.788s
  95th Percentile Latency: 2.946s
  Average RTF: 15.369
  Throughput: 0.4 samples/second

Device: Tesla T4
- Baseline Results:
- Num of requests: 50
- Average Latency: 4.5s
- 95th Percentile Latency: 4.8s
- Average RTF: 25.0
- Throughput: 0.22 samples/second

What to profile:
analysing forward pass, per section

### Inference Pipeline Breakdown
synthesize
- text_to_tokens()
- model()
  - encode_text()
  - decode_mel()
  - vocoder_inference()
- cuda.synchronize()
- post_process()




Bottlenecks in baseline:
- 1. no batching - processing samples individually
- 2. full precision (FP32) - no quantization
  - 1. **Model quantization** (INT8 or mixed precision)
  - Quantize neural TTS models for minimal latency on heterogeneous GPU hardware
- 3. synchronous GPU operations - cuda.synchronize() forces CPU to wait for GPU
- 4. inefficient tensor copies - unnecessary transfers between CPU and GPU
- 5. No caching
  - 3. **KV-cache optimization** for autoregressive generation

From profiling per section:

top per time:
- decode_mel

per memory:
- encode_text
- decode_mel
- vocoder_inference

per gpu utilization: using more than 50%
- decode_mel
- vocoder_inference
- synchronize
- to_cpu


### quantization
- note:
  - when changing verify all operations are casted

added policy detection according to GPU capability
[todo] verify cpu int8 quantization

(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 2 2>&1 | tee logs/p1/baseline_inference_original_3.out



### KV Cache

from logs/profiling_per_section_1_2.png:
- bottleneck: decode_mel more time and gpu utilization
- Autoregressive mel spectrogram generation
aut


short vs long 
(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 20 2>&1 | tee logs/p1/baseline_inference_original_5_short.out

(ml_engineer) ubuntu@ip-172-31-45-48:~/ml_engineer$ uv run baseline_inference_original.py --num_runs 20 2>&1 | tee logs/p1/baseline_inference_original_5_long.out

logs/profiling_per_section_5_cache_long_no_cache.png

With larger texts is more notable the improvement





## Further Improvements
todo: Batching with same improvements
Fallback to CPU if not gpu

[Zhou, Z., Ning, X., Hong, K., Fu, T., Xu, J., Li, S., ... & Wang, Y. (2024). A survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294.]
Explore Optimizations:
- System-level
  - Do not touch the model
  - Inference engines
    - Use open source optimized inference engines, e.g. Triton Inference Server, TorchServe, ...
  - Profile and optimize kernels, e.g. using nsys nvidia tool
  - Distributed systems
- Model-level
  - Model compression techniques, prunning layers, nodes
  - Dynamic inference (Different paths through the model according to input)
- Data-level
  - RAG

