warmup
Benchmarking baseline TTS inference...
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Baseline Results:
  Num of requests: 3
  Average Latency: 0.334s
  95th Percentile Latency: 0.456s
  Average RTF: 921.323
  Throughput: 3.0 samples/second
--------- Exp 6 precision and device---------
Benchmarking baseline TTS inference...
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Quantization precision: tokens torch.int64
Model kv cache: False
Quantization precision: torch.float32, torch.float32, torch.float32
Baseline Results:
  Num of requests: 50
  Average Latency: 0.258s
  95th Percentile Latency: 0.266s
  Average RTF: 711.488
  Throughput: 3.9 samples/second

Profiling Results:
Section: text_to_tokens
  Latency (s): 0.020817900603672244
  Memory delta (MB): 0.0012253095518867925
  GPU utilization (%): 1.528301886792453
Section: encode_text
  Latency (s): 0.025054792188248545
  Memory delta (MB): 0.4104510613207547
  GPU utilization (%): 1.0377358490566038
Section: decode_mel
  Latency (s): 0.027047481176988134
  Memory delta (MB): 0.0009765625
  GPU utilization (%): 0.9433962264150944
Section: vocoder_inference
  Latency (s): 0.022104596191982052
  Memory delta (MB): 0.00048828125
  GPU utilization (%): 0.8679245283018868
Section: synchronize
  Latency (s): 0.020688884663132002
  Memory delta (MB): 0.0
  GPU utilization (%): 0.7547169811320755
Section: to_cpu
  Latency (s): 0.020632932770927
  Memory delta (MB): 0.0
  GPU utilization (%): 1.3962264150943395

Profiling Results:
Section: inference
  Latency (s): 0.283177524242761
  Memory delta (MB): 0.15330188679245282
  GPU utilization (%): 1.6981132075471699
--------- Exp 6 precision and device auto ---------
major: 7 minor: 5
Auto-detected device policy: DevicePolicy(device_str='cuda:0', precision='fp16', autocast_dtype=torch.float16)
prepare_model_for_precision: policy: DevicePolicy(device_str='cuda:0', precision='fp16', autocast_dtype=torch.float16)
prepare_model_for_precision: policy.precision: fp16
prepare_model_for_precision: model.half()
Benchmarking baseline TTS inference...
Synthesizing text of length 12 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 32 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 44 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 76 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 124 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 128 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 229 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 224 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 202 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 307 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 12 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 32 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 44 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 76 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 124 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 128 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 229 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 224 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 202 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 307 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 12 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 32 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 44 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 76 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 124 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 128 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 229 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 224 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
/home/ubuntu/ml_engineer/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/ubuntu/ml_engineer/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/ubuntu/ml_engineer/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/ubuntu/ml_engineer/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 202 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 307 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 12 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 32 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 44 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 76 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 124 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 128 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 229 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 224 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 202 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 307 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 12 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 32 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 44 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 76 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 124 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 128 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 229 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 224 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 202 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Synthesizing text of length 307 with quantization=True, eff_3=True, eff_4=True
Device: cuda
synthesize kv_cache: True
Using precision: torch.float16
Model kv cache: decode_mel_kv_cache
Model kv cache: True
Quantization enabled: torch.float16, torch.float16, torch.float32
Baseline Results:
  Num of requests: 50
  Average Latency: 0.183s
  95th Percentile Latency: 0.188s
  Average RTF: 504.591
  Throughput: 5.5 samples/second

Profiling Results:
Section: text_to_tokens
  Latency (s): 0.000350341796875
  Memory delta (MB): 0.00126953125
  GPU utilization (%): nan
Section: encode_text
  Latency (s): 0.027002620697021484
  Memory delta (MB): 0.269140625
  GPU utilization (%): 0.98
Section: decode_mel
  Latency (s): 0.0273115873336792
  Memory delta (MB): 0.0009765625
  GPU utilization (%): 1.0
Section: vocoder_inference
  Latency (s): 0.022388644218444824
  Memory delta (MB): 0.00048828125
  GPU utilization (%): 1.0
Section: synchronize
  Latency (s): nan
  Memory delta (MB): nan
  GPU utilization (%): nan
Section: to_cpu
  Latency (s): 0.020776777267456054
  Memory delta (MB): 0.0
  GPU utilization (%): 1.0

Profiling Results:
Section: inference
  Latency (s): 0.20371657848358155
  Memory delta (MB): 0.0
  GPU utilization (%): 1.0
--------- Exp 6 ---------
